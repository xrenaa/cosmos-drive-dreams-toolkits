# Copyright (c) 2025, NVIDIA CORPORATION & AFFILIATES.  All rights reserved.
#
# NVIDIA CORPORATION & AFFILIATES and its licensors retain all intellectual property
# and proprietary rights in and to this software, related documentation
# and any modifications thereto.  Any use, reproduction, disclosure or
# distribution of this software and related documentation without an express
# license agreement from NVIDIA CORPORATION & AFFILIATES is strictly prohibited.

import os
import numpy as np
import click
import imageio as imageio_v1
import torch
import cv2
import json
import decord
import ray

from tqdm import tqdm
from pathlib import Path
from termcolor import cprint
from pathlib import Path
from utils.wds_utils import get_sample
from utils.bbox_utils import create_bbox_projection, interpolate_bbox, fix_static_objects
from utils.minimap_utils import create_minimap_projection, simplify_minimap
from utils.pcd_utils import batch_move_points_within_bboxes_sparse, forward_warp_multiframes_sparse_depth_only
from utils.camera.pinhole import PinholeCamera
from utils.camera.ftheta import FThetaCamera
from utils.ray_utils import ray_remote, wait_for_futures

USE_RAY = True

def get_low_fps_indices(high_indices, step=3):
    low_indices = []
    low_indices_times_step = []
    for hi in high_indices:
        # Round to the nearest multiple of 'step'
        low_idx = int(round(hi / step))
        low_idx_times_step = low_idx * step
        low_indices.append(low_idx)
        low_indices_times_step.append(low_idx_times_step)
    return low_indices_times_step, low_indices


def prepare_input(input_root, clip_id, settings, camera_type, post_training, target_resolution, novel_pose_folder):
    """
    Prepare input for rendering.
    Args:
        input_root: the root folder of the input data
        clip_id: the id of the clip
        settings: the settings of the dataset
        camera_type: the type of camera model, 'pinhole' or 'ftheta'
        post_training: if True, use the post-training settings
        target_resolution: the resolution of the output video. It helps us to set the correct camera model.
        novel_pose_folder: the folder name of the novel pose data. If provided, we will render the novel ego trajectory.

    Returns:
        pose_all_frames: the pose of the camera. numpy array of shape (T, 4, 4)
        render_frame_ids: the valid frame ids used for rendering. list of int
        all_object_info: the object info. list of dict
        camera_models: the camera models. dict of camera model objects
    """
    INPUT_POSE_FPS = settings['INPUT_POSE_FPS']

    if post_training: 
        TARGET_RENDER_FPS = settings['POST_TRAINING']['TARGET_RENDER_FPS']
    else:
        TARGET_RENDER_FPS = settings['NOT_POST_TRAINING']['TARGET_RENDER_FPS']

    camera_models = {}

    target_w, target_h = target_resolution
    # generate map projection for the instance buffer
    for camera_name in settings['CAMERAS']:
        print(f"Processing {clip_id} {camera_name}...")
        # load pose
        pose_folder = 'pose' if novel_pose_folder is None else novel_pose_folder
        pose_file = os.path.join(input_root, pose_folder, f"{clip_id}.tar")
        camera_key = f"pose.{camera_name}.npy"
        pose_data = get_sample(pose_file)
        pose_data_this_cam = {k: v for k, v in pose_data.items() if camera_key in k}
        pose_all_frames = np.stack([pose_data_this_cam[k] for k in sorted(pose_data_this_cam.keys())])

        frame_num = pose_all_frames.shape[0]
        render_frame_ids = list(range(0, frame_num, INPUT_POSE_FPS // TARGET_RENDER_FPS))

        # interpolate bbox from 10Hz to 30Hz
        all_object_info_file = os.path.join(input_root, 'all_object_info', f"{clip_id}.tar")
        all_object_info = get_sample(all_object_info_file)

        # fix static objects in jitter
        all_object_info = fix_static_objects(all_object_info)

        # interpolate bbox if box label FPS is lower than TARGET_RENDER_FPS
        all_object_info = interpolate_bbox(all_object_info, render_frame_ids)

        # read intrinsic and build camera model.
        if camera_type == "pinhole":
            intrinsic_file = os.path.join(input_root, f'{camera_type}_intrinsic', f"{clip_id}.tar")
            intrinsic_data = get_sample(intrinsic_file)
            intrinsic_this_cam = intrinsic_data[f"{camera_type}_intrinsic.{camera_name}.npy"]
            camera_model = PinholeCamera.from_numpy(intrinsic_this_cam, device='cpu')

            rescale_h = target_h / camera_model.height
            rescale_w = target_w / camera_model.width
            camera_model.rescale(rescale_h, rescale_w)

        elif camera_type == "ftheta":
            intrinsic_file = os.path.join(input_root, f'{camera_type}_intrinsic', f"{clip_id}.tar")

            # 3-rd party dataset does not have ftheta intrinsic, we can use the default one from RDS-HQ
            if os.path.exists(intrinsic_file):
                intrinsic_data = get_sample(intrinsic_file)
                intrinsic_this_cam = intrinsic_data[f"{camera_type}_intrinsic.{camera_name}.npy"]

            else:
                cprint(f"Ftheta intrinsic file does not exist: {intrinsic_file}", 'red')
                cprint(f"===> So we will use default ftheta intrinsic for rendering", 'yellow', attrs=['bold'])
                intrinsic_file = 'config/default_ftheta_intrinsic.tar'
                camera_name_in_rds_hq = settings['CAMERAS_TO_RDS_HQ'][camera_name]    
                intrinsic_data = get_sample(intrinsic_file)
                intrinsic_this_cam = intrinsic_data[f"{camera_type}_intrinsic.{camera_name_in_rds_hq}.npy"]

            camera_model = FThetaCamera.from_numpy(intrinsic_this_cam, device='cpu')
            rescale_h = target_h / camera_model.height
            rescale_w = target_w / camera_model.width

            assert abs(rescale_h - rescale_w) < 0.01 # only handle the case that height is downsampled by the same ratio, required by the ftheta camera model
            camera_model.rescale(rescale_h)
        else:
            raise ValueError(f"Invalid camera type: {camera_type}")

        camera_models[camera_name] = camera_model

    return pose_all_frames, render_frame_ids, all_object_info, camera_models

def prepare_output(
        full_video, 
        render_frame_ids, 
        render_name, 
        settings, 
        output_root, 
        output_folder, 
        clip_id, 
        camera_name, 
        target_resolution, 
        crop_resolution, 
        post_training
    ):
    """
    Cut full video into small clips for cosmos training / inference and save them.

    Args:
        full_video: the full video to be cut. numpy array of shape (T, H, W, 3)
        render_frame_ids: the frame ids to be rendered. list of int
        render_name: the name of the rendered video. str. 'hdmap', 'lidar', or 'rgb'
        settings: the settings of the dataset
        output_root: the root folder of the output data
        output_folder: the folder name of the output data
        clip_id: the id of the clip
        camera_name: the name of the camera
    """
    if post_training: 
        TARGET_RENDER_FPS = settings['POST_TRAINING']['TARGET_RENDER_FPS']
        CUT_LEN = settings['POST_TRAINING']['CUT_LEN']
        OVERLAP = settings['POST_TRAINING']['OVERLAP']
    else:
        TARGET_RENDER_FPS = settings['NOT_POST_TRAINING']['TARGET_RENDER_FPS']
        CUT_LEN = settings['NOT_POST_TRAINING']['CUT_LEN']
        OVERLAP = settings['NOT_POST_TRAINING']['OVERLAP']

    # sometimes the full_video does not match the target resolution, e.g. 720 * 1277, we need to resize it
    if full_video.shape[1] != target_resolution[1] or full_video.shape[2] != target_resolution[0]:
        print(f"Resizing {clip_id} {render_name} video from {full_video.shape[1]}x{full_video.shape[2]} to {target_resolution[1]}x{target_resolution[0]}...")
        full_video = np.stack([cv2.resize(frame, target_resolution, interpolation=cv2.INTER_LANCZOS4) for frame in full_video], axis=0)

    if crop_resolution != target_resolution:
        crop_w, crop_h = crop_resolution
        target_w, target_h = target_resolution
        crop_x = (target_w - crop_w) // 2
        crop_y = (target_h - crop_h) // 2
        full_video = full_video[:, crop_y:crop_y+crop_h, crop_x:crop_x+crop_w]

    output_root_p = Path(output_root)
    (output_root_p / camera_name / output_folder / clip_id).mkdir(parents=True, exist_ok=True)
    for cur_idx, i in enumerate(range(3, len(render_frame_ids), CUT_LEN - OVERLAP)):
        if i + CUT_LEN > len(render_frame_ids):
            continue

        full_video_cut = full_video[i:i+CUT_LEN]

        # save HD map condition video
        map_writer = imageio_v1.get_writer(
            output_root_p / camera_name / output_folder / clip_id / f"{cur_idx}_{render_name}_cond.mp4",
            fps=TARGET_RENDER_FPS,
            codec="libx264",
            macro_block_size=None,  # This makes sure num_frames is correct (by default it is rounded to 16x).
            ffmpeg_params=[
                "-crf", "18",     # Lower CRF for higher quality (0-51, lower is better)
                "-preset", "slow",   # Slower preset for better compression/quality
                "-pix_fmt", "yuv420p", # Ensures wide compatibility
            ],
        )
        for local_frame_idx in range(len(full_video_cut)):
            map_writer.append_data(full_video_cut[local_frame_idx])
        map_writer.close()


@ray_remote(use_ray=USE_RAY, num_gpus=0)
def render_sample_hdmap(
    input_root: str,
    output_root: str,
    clip_id: str,
    settings: dict,
    camera_type: str,
    output_folder: str,
    post_training: bool = False,
    novel_pose_folder: str = None,
    target_resolution: tuple[int, int] = (1280, 720),
    crop_resolution: tuple[int, int] = (1280, 704),
):
    minimap_types = settings['MINIMAP_TYPES']

    pose_all_frames, render_frame_ids, all_object_info, camera_models = \
        prepare_input(input_root, clip_id, settings, camera_type, post_training, target_resolution, novel_pose_folder)
    minimap_wds_files = [
        os.path.join(input_root, f"3d_{minimap_type}", f"{clip_id}.tar") for minimap_type in minimap_types
    ]

    for camera_name, camera_model in camera_models.items():
        minimaps_projection_merged = np.zeros((len(render_frame_ids), camera_model.height, camera_model.width, 3), dtype=np.uint8)
        for minimap_wds_file in minimap_wds_files:
            minimap_data_wo_meta_info, minimap_name = simplify_minimap(minimap_wds_file)

            # all static labels
            minimap_projection = create_minimap_projection(
                minimap_name,
                minimap_data_wo_meta_info,
                pose_all_frames[render_frame_ids],
                camera_model
            )
            minimaps_projection_merged = np.maximum(minimaps_projection_merged, minimap_projection)

        # add bounding box projection to the minimap
        bounding_box_projection = create_bbox_projection(
            all_object_info,
            pose_all_frames,
            render_frame_ids,
            camera_model,
        )
        minimaps_projection_merged = np.maximum(minimaps_projection_merged, bounding_box_projection)

        prepare_output(
            minimaps_projection_merged, 
            render_frame_ids, 
            'hdmap', 
            settings,
            output_root, 
            output_folder, 
            clip_id, 
            camera_name, 
            target_resolution, 
            crop_resolution, 
            post_training
        )


@ray_remote(use_ray=USE_RAY, num_gpus=1, num_cpus=2)
def render_sample_lidar(
    input_root: str,
    output_root: str,
    clip_id: str,
    settings: dict,
    camera_type: str,
    output_folder: str,
    post_training: bool = False,
    novel_pose_folder: str = None,
    target_resolution: tuple[int, int] = (1280, 720),
    crop_resolution: tuple[int, int] = (1280, 704),
    accumulate_lidar_frames: int = 2,
):
    INPUT_POSE_FPS = settings['INPUT_POSE_FPS']
    INPUT_LIDAR_FPS = settings['INPUT_LIDAR_FPS']

    target_w, target_h = target_resolution
    pose_all_frames, render_frame_ids, all_object_info, camera_models = \
        prepare_input(input_root, clip_id, settings, camera_type, post_training, target_resolution, novel_pose_folder)

    frame_num = len(render_frame_ids)

    for camera_name, camera_model in camera_models.items():
        # load lidar data
        lidar_tar = os.path.join(input_root, 'lidar_raw', f"{clip_id}.tar")
        lidar_data = get_sample(lidar_tar) # 'xxxxxx.xyz' & 'xxxxxx.lidar_to_world'

        # generate lidar depth
        lidar_depth_list = []
        interp_rate = INPUT_POSE_FPS // INPUT_LIDAR_FPS
        for frame_idx in render_frame_ids:
            accumulate_idx = [min(max(0, frame_idx + i * interp_rate), frame_num-1) for i in range(-accumulate_lidar_frames, accumulate_lidar_frames+1)]
            low_fps_frame_indices, _ = get_low_fps_indices(accumulate_idx, step=interp_rate)

            frame_ids = [
                int(key.split('.')[0])
                for key in lidar_data.keys()
                if key.endswith('.lidar_raw.npz') and key.split('.')[0].isdigit()
            ]
            max_lidar_idx = max(frame_ids)
            low_fps_frame_indices = [min(max_lidar_idx, idx) for idx in low_fps_frame_indices]

            accumulated_points, original_bbox_dicts, moved_bbox_dicts = [], [], []

            for i in range(len(accumulate_idx)):
                lidar_points = lidar_data[f'{low_fps_frame_indices[i]:06d}.lidar_raw.npz']['xyz'].astype(np.float32).reshape(-1, 3)

                distances = np.linalg.norm(lidar_points, axis=1)
                threshold = 3.0  
                lidar_points = lidar_points[distances >= threshold]
                lidar_to_world = lidar_data[f'{low_fps_frame_indices[i]:06d}.lidar_raw.npz']['lidar_to_world']
                lidar_points_world = (lidar_to_world @ np.concatenate([lidar_points, np.ones([lidar_points.shape[0], 1])], axis=1).T).T[:, :3]
                accumulated_points.append(torch.tensor(lidar_points_world, device='cuda'))
                original_bbox_dicts.append(all_object_info[f'{low_fps_frame_indices[i]:06d}.all_object_info.json'])
                moved_bbox_dicts.append(all_object_info[f'{frame_idx:06d}.all_object_info.json'])

            accumulated_points_corrected, _ = batch_move_points_within_bboxes_sparse(
                accumulated_points,
                original_bbox_dicts,
                moved_bbox_dicts,
                torch.tensor(low_fps_frame_indices),
                dynamic_only=True,
                bbox_scale=1.2, # enlarge bbox to include all points
            )
            lengths_tensor = torch.tensor([t.shape[0] for t in accumulated_points_corrected], device='cuda')
            cat_points = torch.cat(accumulated_points_corrected, dim=0)
            w2cs = torch.inverse(torch.tensor(pose_all_frames[frame_idx])).unsqueeze(0).cuda()
            intrinsics = torch.tensor(camera_model.intrinsics).unsqueeze(0).to(cat_points)
            depth, mask = forward_warp_multiframes_sparse_depth_only(
                w2cs,
                intrinsics,
                buffer_points=cat_points.cuda(),
                buffer_length=lengths_tensor.cuda(),
                target_h=target_h, 
                target_w=target_w,
                center_only=False,
                is_ftheta=(camera_type == 'ftheta'),
                expanded_kernel=4
            )            
            depth = depth.squeeze(0).squeeze(0).cpu().numpy()
            lidar_depth_list.append(depth)
        lidar_depth = np.stack(lidar_depth_list, axis=0) # T, H, W
        # clamp to 75
        lidar_depth_vis = np.clip(lidar_depth, 0, 75) / 75.0 * 255
        lidar_depth_vis = lidar_depth_vis.astype(np.uint8)
        lidar_depth_vis = np.stack([lidar_depth_vis] * 3, axis=-1) # T, H, W, 3

        # prepare output
        prepare_output(
            lidar_depth_vis,
            render_frame_ids,
            'lidar',
            settings,
            output_root,
            output_folder,
            clip_id,
            camera_name,
            target_resolution,
            crop_resolution,
            post_training
        )


@ray_remote(use_ray=USE_RAY, num_gpus=0)
def render_sample_rgb(
    input_root: str,
    output_root: str,
    clip_id: str,
    settings: dict,
    camera_type: str,
    output_folder: str,
    post_training: bool = False,
    novel_pose_folder: str = None,
    target_resolution: tuple[int, int] = (1280, 720),
    crop_resolution: tuple[int, int] = (1280, 704),
):
    """
    This function is used to render / sample the RGB video for post-training use.
    """
    target_w, target_h = target_resolution
    pose_all_frames, render_frame_ids, all_object_info, camera_models = \
        prepare_input(input_root, clip_id, settings, camera_type, post_training, target_resolution, novel_pose_folder)

    for camera_name, camera_model in camera_models.items():
        # load rgb
        rgb_file = os.path.join(input_root, f'{camera_type}_{camera_name}', f"{clip_id}.mp4")
        # load all frames
        vr = decord.VideoReader(rgb_file)
        num_frames = len(vr)
        all_frames = [vr[i] for i in range(num_frames)]
        # resize all frames to target_resolution
        all_frames_resized = []
        for frame_read in all_frames:
            try:
                frame = frame_read.asnumpy()
            except AttributeError:
                frame = frame_read.numpy()
            frame_resized = cv2.resize(frame, (target_w, target_h), interpolation=cv2.INTER_LANCZOS4)
            all_frames_resized.append(frame_resized)
        all_frames_resized = np.stack(all_frames_resized, axis=0)

        prepare_output(
            all_frames_resized,
            render_frame_ids,
            'rgb',
            settings,
            output_root,
            output_folder,
            clip_id,
            camera_name,
            target_resolution,
            crop_resolution,
            post_training
        )
            
@click.command()
@click.option("--input_root", '-i', type=str, help="the root folder of the input data")
@click.option("--output_root", '-o', type=str, help="the root folder of the output data")
@click.option("--dataset", "-d", type=str, default="rds_hq", help="the dataset name, 'rds_hq' or 'waymo' or 'waymo_mv', see the config in settings.json")
@click.option("--camera_type", "-c", type=str, default="ftheta", help="the type of camera model, 'pinhole' or 'ftheta'")
@click.option("--skip", "-s", multiple=True, help="can be 'hdmap' or 'lidar'")
@click.option("--output_folder", "-f", type=str, default="render", help="Output folder")
@click.option("--post_training", "-p", type=bool, default=False, help="if True, output the RGB video for post-training")
@click.option("--num", "-n", type=int, default=-1, help="num clips to process")
@click.option("--novel_pose_folder", "-np", type=str, default=None, help="the folder name of the novel pose data. If provided, we will render the novel ego trajectory")
def main(input_root, output_root, dataset, camera_type, skip, output_folder, post_training, num, novel_pose_folder):
    if skip is not None:
        assert all(s in ['hdmap', 'lidar'] for s in skip), "skip must be in ['hdmap', 'lidar']"

    if post_training: # for post-training only
        assert dataset in ['waymo', 'waymo_mv'], "post_training is only supported for waymo dataset"
        assert camera_type == 'pinhole', "post_training is only supported for pinhole camera"

    # Load settings
    with open(f'config/dataset_{dataset}.json', 'r') as file:
        settings = json.load(file)

    # if novel_pose_folder is provided, we also change the output folder
    if novel_pose_folder is not None:
        output_folder = f'{output_folder}_{novel_pose_folder}'

    # get all clip ids
    input_root_p = Path(input_root)
    pose_folder = 'pose' if novel_pose_folder is None else novel_pose_folder
    clip_list = (input_root_p / pose_folder).rglob('*.tar')
    clip_list = [c.stem for c in clip_list]
    if num > 0:
        clip_list = clip_list[:num]

    # shuffle the clip list
    np.random.seed(0)
    np.random.shuffle(clip_list)

    if USE_RAY:
        ray.init()
        futures = []
        if 'hdmap' not in skip:
            futures.extend([render_sample_hdmap.remote(input_root, output_root, clip_id, settings, camera_type, output_folder, post_training, novel_pose_folder) for clip_id in clip_list])
        if 'lidar' not in skip:
            futures.extend([render_sample_lidar.remote(input_root, output_root, clip_id, settings, camera_type, output_folder, post_training, novel_pose_folder) for clip_id in clip_list])
        if post_training:
            futures.extend([render_sample_rgb.remote(input_root, output_root, clip_id, settings, camera_type, output_folder, post_training, novel_pose_folder) for clip_id in clip_list])
        wait_for_futures(futures)
    else:
        for clip_id in tqdm(clip_list):
            if 'hdmap' not in skip:
                render_sample_hdmap(input_root, output_root, clip_id, settings, camera_type, output_folder, post_training, novel_pose_folder)
            if 'lidar' not in skip:
                render_sample_lidar(input_root, output_root, clip_id, settings, camera_type, output_folder, post_training, novel_pose_folder)
            if post_training:
                render_sample_rgb(input_root, output_root, clip_id, settings, camera_type, output_folder, post_training, novel_pose_folder)

if __name__ == "__main__":
    main()